{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "### 1. Noise reduction techniques\n",
    "Special characters are often present in text data. In NLP, these characters can affect the performance of machine learning algorithms and can make the text data difficult to process. Depending on the task, it is important to remove or replace these characters before processing the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harper is a good girl\n",
      "Harper is a nice girl!\n",
      "Harper is the best girl!\n",
      "H*rp*r *s * g**d g*rl!\n",
      "Harper ** a good girl!\n",
      "H****r is a g**d g**l!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = 'Harper is a good girl!'\n",
    "\n",
    "# remove all non-word and non-space characters\n",
    "print(re.sub(r'[^\\w\\s]', '', sentence))\n",
    "\n",
    "# replace all occurrences of 'good' with 'nice'\n",
    "print(re.sub(r'\\bgood\\b', 'nice', sentence))\n",
    "\n",
    "# replace 'a good' with 'the best'\n",
    "print(re.sub('..g.*d', 'the best', sentence))\n",
    "\n",
    "# Replaces vowels with '*'\n",
    "print(re.sub(r'[aeiou]', '*', sentence)) \n",
    "\n",
    "# replace the exact number of occurrences to *, observe the \\B and \\b\n",
    "print(re.sub(r'\\b\\w{2}\\b', '**', sentence))\n",
    "print(re.sub(r'\\B\\w{2}\\B', '**', sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** contact number is 6791       1744.\n",
      "NTU contact number is XXXX       XXXX.\n",
      "NTU contact number is 6791 1744.\n",
      "NTU contact number is 1744       6791.\n",
      "NTU contact number is 1744 6791.\n"
     ]
    }
   ],
   "source": [
    "sentence = 'NTU contact number is 6791       1744.'\n",
    "\n",
    "# replace captial letters with '*'\n",
    "print(re.sub(r'[A-Z]', '*', sentence))\n",
    "\n",
    "# replace all digits with 'X'\n",
    "print(re.sub(r'\\d', 'X', sentence))\n",
    "\n",
    "# Replaces multiple whitespaces with a single space\n",
    "print(re.sub(r'\\s+', ' ', sentence))\n",
    "\n",
    "# swap the order of the 4-digit pairs\n",
    "print(re.sub(r'(\\d{4})(\\s+)(\\d{4})', r'\\3\\2\\1', sentence))\n",
    "\n",
    "# swap the order of the 4-digit pairs and disregard the multiple whitespaces pattern\n",
    "print(re.sub(r'(\\d{4})\\s+(\\d{4})', r'\\2 \\1', sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization\n",
    "Tokenization is the process of splitting text into smaller units such as words, sentences, or subwords. You will learn more about handling OOV words using subword tokenization in the later weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Natural Language Processing is amazing!', \"Let's learn it step by step.\"]\n",
      "Word Tokens: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'learn', 'it', 'step', 'by', 'step', '.']\n",
      "Original Word: Natural, Processed Word: Natural\n",
      "Original Word: Language, Processed Word: Language\n",
      "Original Word: Processing, Processed Word: Processing\n",
      "Original Word: is, Processed Word: is\n",
      "Original Word: amazing, Processed Word: amazing\n",
      "Original Word: !, Processed Word: <UNK>\n",
      "Original Word: Let, Processed Word: <UNK>\n",
      "Original Word: 's, Processed Word: <UNK>\n",
      "Original Word: learn, Processed Word: <UNK>\n",
      "Original Word: it, Processed Word: <UNK>\n",
      "Original Word: step, Processed Word: <UNK>\n",
      "Original Word: by, Processed Word: <UNK>\n",
      "Original Word: step, Processed Word: <UNK>\n",
      "Original Word: ., Processed Word: <UNK>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is amazing! Let's learn it step by step.\"\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "# Replacement of OOV words with UNK_TOKEN\n",
    "UNK_TOKEN = '<UNK>'\n",
    "\n",
    "# Vocabulary containing known words\n",
    "vocabulary = set('Natural Language Processing is amazing'.split())\n",
    "\n",
    "# Function to handle OOV words\n",
    "def handle_oov(word):\n",
    "    return word if word in vocabulary else UNK_TOKEN\n",
    "\n",
    "for word in word_tokens:\n",
    "    print(f'Original Word: {word}, Processed Word: {handle_oov(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stop word removal\n",
    "Stop words are the most common words in a language that do not carry much meaning. They include articles (the, a, an), pronouns (he, she, it), conjunctions (and, or, but), prepositions (in, on, at), and other words that are used frequently but do not carry much meaning. \n",
    "\n",
    "In natural language processing, stop words are removed from text data to improve the accuracy of analysis and to reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'showing', 'remove', 'stop', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = \"This is an example showing how to remove stop words.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalization\n",
    "Converts text into a standard format, such as lowercasing, expanding contractions, and stemming/lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm learning nlp, and it isn't easy!\n",
      "I'M LEARNING NLP, AND IT ISN'T EASY!\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm learning NLP, and it isn't easy!\"\n",
    "\n",
    "# casing of words in the text\n",
    "print(text.lower())\n",
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning NLP, and it is not easy!\n"
     ]
    }
   ],
   "source": [
    "from contractions import fix\n",
    "\n",
    "# Expands abbreviation\n",
    "normalized_text = fix(text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'NLP', ',', 'and', 'it', 'is', 'not', 'easy', '!']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(normalized_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'learn', 'nlp', ',', 'and', 'it', 'is', 'not', 'easi', '!']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'learn', 'NLP', ',', 'and', 'it', 'be', 'not', 'easy', '!']\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Specify 'v' for verbs\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]  \n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lemmatizer to work as intended, we need to give the lemmatizer the context of each word. This is achieved through POS tagging, which will be covered in greater detail next week. The default POS tagger assumes all words to be nouns if no context is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('amazing', 'JJ'), ('and', 'CC'), ('fun', 'NN'), ('to', 'TO'), ('learn', 'VB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging labels each word with its grammatical role.\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"NLP is amazing and fun to learn.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'n'), ('is', 'v'), ('amazing', 'a'), ('and', None), ('fun', 'n'), ('to', None), ('learn', 'v'), ('.', None)]\n",
      "NLP be amazing and fun to learn .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "    \n",
    "wordnet_tagged = [(word, pos_tagger(pos)) for (word, pos) in pos_tags]\n",
    "print(wordnet_tagged)\n",
    "\n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:       \n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vectorization \n",
    "Converting a text, sentence, or word into a numerical representation that can be used for further processing. This can be done using techniques such as Bag-of-Words, N-grams, and Word Embeddings. In this notebook, we will focus on Bag-of-Words and N-grams.  You will learn the others in the later weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never willing see fall', 'never hesitating rock']\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words\n",
    "text = \"\"\"\n",
    "/W Never willing to see you fall. /s Never hesitating to be your rock.\n",
    "\"\"\"\n",
    "\n",
    "filtered_text = []\n",
    "for sent in sent_tokenize(text):\n",
    "    sent = re.sub(r'/W', ' ', sent)\n",
    "    sent = re.sub(r'/s+', ' ', sent)\n",
    "    sent = re.sub('[^a-zA-Z\"]', ' ', sent)\n",
    "    filtered_text.append(' '.join([word.lower() for word in word_tokenize(sent) if word.lower() not in stop_words]))\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[1 0 1 0 1 1]\n",
      " [0 1 1 1 0 0]]\n",
      "\n",
      "Vocabulary:\n",
      "['fall' 'hesitating' 'never' 'rock' 'see' 'willing']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data to get the BoW representation\n",
    "X = vectorizer.fit_transform(filtered_text)\n",
    "\n",
    "# Get the list of unique words in the vocabulary\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the BoW matrix\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"\\nVocabulary:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I am the one who knocks\n",
      "\n",
      "Unigram:\n",
      "['I', 'am', 'the', 'one', 'who', 'knocks']\n",
      "\n",
      "Bigram:\n",
      "['I am', 'am the', 'the one', 'one who', 'who knocks']\n",
      "\n",
      "Trigram:\n",
      "['I am the', 'am the one', 'the one who', 'one who knocks']\n"
     ]
    }
   ],
   "source": [
    "# N-grams\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Sample text\n",
    "text = \"I am the one who knocks\"\n",
    "# Print the generated n-grams\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nUnigram:\")\n",
    "print(generate_ngrams(text, 1))\n",
    "print(\"\\nBigram:\")\n",
    "print(generate_ngrams(text, 2))\n",
    "print(\"\\nTrigram:\")\n",
    "print(generate_ngrams(text, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice for the week\n",
    "Explore these preprocessing techniques on the given movie reviews. Below are comments and codes to guide you through. Add and/or modify the code when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data: ['Excellent performance by Mary KAy Place Steve Sandvoss Jacqueline Bissett and Rebekah Johnson Superb story that reels you into the movie emotional yet lighthearted I own this movie and everyone that I have shared it with loves it Great for mixed company 18 crowdbr br Nice production not a cheap budget well organized and keeps your interest Uses dome newer ideas for flashbacks and at one point keeps the viewer at the edge of their seatbr br No matter what walk of life a viewer is from they will buyin to one or more viewpoints in this filmbr br Would love to see a sequel', 'This movie is totally wicked It is really great to see MJH in a different role than her Sabrina character The plot is totally cool and the characters are excellently written Definitely one of the best movies']\n",
      "\n",
      "\n",
      "Vocabulary: ['best', 'bissett', 'br', 'budget', 'character', 'characters', 'cheap', 'company', 'cool', 'definitely', 'different', 'dome', 'edge', 'emotional', 'everyone', 'excellent', 'excellently', 'flashbacks', 'great', 'ideas', 'interest', 'jacqueline', 'johnson', 'kay', 'keeps', 'life', 'love', 'loves', 'mary', 'matter', 'mixed', 'mjh', 'movie', 'movies', 'newer', 'nice', 'one', 'organized', 'performance', 'place', 'plot', 'point', 'production', 'really', 'rebekah', 'reels', 'role', 'sabrina', 'sandvoss', 'see', 'sequel', 'shared', 'steve', 'story', 'superb', 'totally', 'uses', 'viewer', 'viewpoints', 'walk', 'well', 'wicked', 'would', 'written', 'yet']\n",
      "Preprocessed Review 1: Excellent performance by Mary KAy Place Steve Sandvoss Jacqueline Bissett and Rebekah Johnson Superb story that reels you into the movie emotional yet lighthearted I own this movie and everyone that I have shared it with loves it Great for mixed company 18 crowdbr br Nice production not a cheap budget well organized and keeps your interest Uses dome newer ideas for flashbacks and at one point keeps the viewer at the edge of their seatbr br No matter what walk of life a viewer is from they will buyin to one or more viewpoints in this filmbr br Would love to see a sequel\n",
      "Preprocessed Review 2: This movie is totally wicked It is really great to see MJH in a different role than her Sabrina character The plot is totally cool and the characters are excellently written Definitely one of the best movies\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries/packages/modules.\n",
    "# !pip install contractions\n",
    "# download necessary resources\n",
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from contractions import fix\n",
    "import contractions\n",
    "\n",
    "# raw examples\n",
    "raw_data = [\n",
    "    \"Excellent performance by Mary KAy Place, Steve Sandvoss, Jacqueline Bissett and Rebekah Johnson. Superb story that reels you into the movie, emotional, yet light-hearted. I own this movie, and everyone that I've shared it with, loves it. Great for mixed company, 18+ crowd.<br /><br />Nice production, not a cheap budget, well organized, and keeps your interest. Uses dome newer ideas, for flashbacks, and at one point keeps the viewer at the edge of their seat.<br /><br />No matter what walk of life a viewer is from, they will buy-in to one or more viewpoints in this film.<br /><br />Would love to see a sequel!\",\n",
    "    \"This movie is totally wicked! It's really great to see MJH in a different role than her Sabrina character! The plot is totally cool, and the characters are excellently written. Definitely one of the best movies!!\"\n",
    "]\n",
    "\n",
    "cleaned_data = []\n",
    "# noise reduction, normalization, tokenization, stopword removal\n",
    "\n",
    "# normalization and noise reduction\n",
    "\n",
    "# Expand contractions & clean text in one pass\n",
    "cleaned_data = [\n",
    "    re.sub(r'[^\\w\\s]', '', contractions.fix(text))  # expand â†’ remove punctuation\n",
    "    for text in raw_data\n",
    "]\n",
    "\n",
    "# Output\n",
    "print(\"Cleaned Data:\", cleaned_data)\n",
    "print('\\n')  \n",
    "\n",
    "# tokenization and stopword removal\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# One-pass: build vocabulary directly from raw_data\n",
    "vocabulary = {\n",
    "    word.lower()\n",
    "    for text in raw_data\n",
    "    for sentence in sent_tokenize(text)\n",
    "    for word in word_tokenize(sentence)\n",
    "    if word.isalpha() and word.lower() not in stop_words         # keep only alphabetic tokens (drops punctuation/numbers)\n",
    "}\n",
    "\n",
    "# (Optional) see it sorted for readability\n",
    "print(\"Vocabulary:\", sorted(vocabulary))\n",
    "\n",
    "for review in raw_data:\n",
    "    pass\n",
    "\n",
    "# observe your output with the original text, you might modify the code for better illustration\n",
    "for idx, review in enumerate(cleaned_data):\n",
    "    print(f\"Preprocessed Review {idx + 1}: {review}\")\n",
    "\n",
    "# vectorization\n",
    "for review in cleaned_data:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
