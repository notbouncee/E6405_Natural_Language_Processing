{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a884065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6c1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fa5d5",
   "metadata": {},
   "source": [
    "### 1. Seq2Seq Model\n",
    "\n",
    "The Seq2Seq model is a type of neural network architecture that is used for machine translation tasks. It consists of an encoder and a decoder. The encoder takes in a sequence of words and produces a fixed-size vector representation of the input sequence. The decoder then takes in this vector representation and produces a sequence of words that is likely to be a translation of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e309038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) \n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109b8123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3004\n",
      "Epoch [2/10], Loss: 2.1753\n",
      "Epoch [3/10], Loss: 2.0519\n",
      "Epoch [4/10], Loss: 1.9382\n",
      "Epoch [5/10], Loss: 1.8353\n",
      "Epoch [6/10], Loss: 1.7334\n",
      "Epoch [7/10], Loss: 1.6415\n",
      "Epoch [8/10], Loss: 1.5559\n",
      "Epoch [9/10], Loss: 1.4677\n",
      "Epoch [10/10], Loss: 1.3946\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "input_size = 10\n",
    "output_size = 10\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size ).to(device)\n",
    "decoder = DecoderRNN(hidden_size , output_size).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, 10, (batch_size, input_size))\n",
    "tgt_data = torch.randint(1, 10, (batch_size, output_size))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(src_data)\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, tgt_data)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        tgt_data.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5871151",
   "metadata": {},
   "source": [
    "### 2. Bahdanau Attention Mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907d4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b534530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2881\n",
      "Epoch [2/10], Loss: 2.1734\n",
      "Epoch [3/10], Loss: 1.9813\n",
      "Epoch [4/10], Loss: 1.8295\n",
      "Epoch [5/10], Loss: 1.7497\n",
      "Epoch [6/10], Loss: 1.6032\n",
      "Epoch [7/10], Loss: 1.5098\n",
      "Epoch [8/10], Loss: 1.4343\n",
      "Epoch [9/10], Loss: 1.3272\n",
      "Epoch [10/10], Loss: 1.2384\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "input_size = 10\n",
    "output_size = 10\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size ).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size , output_size).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, 10, (batch_size, input_size))\n",
    "tgt_data = torch.randint(1, 10, (batch_size, output_size))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(src_data)\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, tgt_data)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        tgt_data.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bca12",
   "metadata": {},
   "source": [
    "### 3. Transformer and its components\n",
    "The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.\n",
    "\n",
    "The MultiHeadAttention code initializes the module with input parameters and linear transformation layers. It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads. The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5bc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The init constructor checks whether the provided d_model is divisible by the number of heads (num_heads). \n",
    "    It sets up the necessary parameters and creates linear transformations for\n",
    "    query(W_q), key(W_k) and output(W_o) projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    \"\"\"\n",
    "     The scaled_dot_product_attention function computes the scaled dot-product attention given the \n",
    "     query (Q), key (K), and value (V) matrices. It uses the scaled dot product formula, applies a mask if \n",
    "     provided, and computes the attention probabilities using the softmax function.\n",
    "    \"\"\"    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    \"\"\"\n",
    "    The split_heads and combine_heads functions handle the splitting and combining of the attention heads.\n",
    "    They reshape the input tensor to allow parallel processing of different attention heads.\n",
    "    \"\"\"\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "     The forward function takes input query (Q), key (K), and value (V) tensors, \n",
    "     applies linear transformations, splits them into multiple heads, performs scaled dot-product attention,\n",
    "     combines the attention heads, and applies a final linear transformation.\n",
    "    \"\"\"    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88e83c",
   "metadata": {},
   "source": [
    "The PositionWiseFeedForward class extends PyTorch’s nn.Module and implements a position-wise feed-forward network. The class initializes with two linear transformation layers and a ReLU activation function. The forward method applies these transformations and activation function sequentially to compute the output. This process enables the model to consider the position of input elements while making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1471af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionWiseFeedForward module. It takes d_model as the input dimension and d_ff \n",
    "    as the hidden layer dimension. \n",
    "    Two linear layers (fc1 and fc2) are defined with ReLU activation in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x, applies the first linear transformation (fc1), \n",
    "    applies the ReLU activation, and then applies the second linear transformation (fc2). \n",
    "    The output is the result of the second linear transformation.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7024c",
   "metadata": {},
   "source": [
    "Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.\n",
    "\n",
    "The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values. The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term. The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bc690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the PositionalEncoding module. \n",
    "    It takes d_model as the dimension of the model and max_seq_length as the maximum sequence length. \n",
    "    It computes the positional encoding matrix (pe) using sine and cosine functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x and adds the positional encoding to it. \n",
    "    The positional encoding is truncated to match the length of the input sequence (x.size(1)).\n",
    "    \"\"\"    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6173c7",
   "metadata": {},
   "source": [
    "An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.\n",
    "\n",
    "The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc16eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the EncoderLayer module. \n",
    "    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads), \n",
    "    d_ff (dimension of the feedforward network), and dropout (dropout rate). \n",
    "    It creates instances of MultiHeadAttention, PositionWiseFeedForward, and nn.LayerNorm. \n",
    "    Dropout is also defined as a module.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x and a mask. \n",
    "    It applies the self-attention mechanism (self.self_attn), adds the residual connection \n",
    "    with layer normalization, applies the position-wise feedforward network (self.feed_forward),\n",
    "    and again adds the residual connection with layer normalization. \n",
    "    Dropout is applied at both the self-attention and feedforward stages.\n",
    "    The mask parameter is used to mask certain positions during the self-attention step, \n",
    "    typically to prevent attending to future positions in a sequence.\n",
    "    \"\"\"\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ce680",
   "metadata": {},
   "source": [
    "A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers.\n",
    "\n",
    "The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81934a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the DecoderLayer module. \n",
    "    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads), \n",
    "    d_ff (dimension of the feedforward network), and dropout (dropout rate). \n",
    "    It creates instances of MultiHeadAttention for both self-attention (self.self_attn) and cross-attention\n",
    "    (self.cross_attn), PositionWiseFeedForward, and nn.LayerNorm. Dropout is also defined as a module.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x, the output from the encoder (enc_output), \n",
    "    and masks for the source (src_mask) and target (tgt_mask). It applies the self-attention mechanism, \n",
    "    adds the residual connection with layer normalization, applies the cross-attention mechanism with the \n",
    "    encoder's output, adds another residual connection with layer normalization, applies the position-wise \n",
    "    feedforward network, and adds a final residual connection with layer normalization. \n",
    "    Dropout is applied at each stage.\n",
    "    \"\"\"\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c8e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the Transformer module. \n",
    "    It takes several hyperparameters, including vocabulary sizes for the source and target languages \n",
    "    (src_vocab_size and tgt_vocab_size), model dimension (d_model), number of attention heads (num_heads), \n",
    "    number of layers (num_layers), dimension of the feedforward network (d_ff), maximum sequence length \n",
    "    (max_seq_length), and dropout rate (dropout).\n",
    "    It sets up embeddings for both the encoder and decoder (encoder_embedding and decoder_embedding), \n",
    "    a positional encoding module (positional_encoding), encoder layers (encoder_layers), \n",
    "    decoder layers (decoder_layers), a linear layer (fc), and dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \"\"\"\n",
    "     The generate_mask function creates masks for the source and target sequences. \n",
    "     It generates a source mask by checking if the source sequence elements are not equal to 0. \n",
    "     For the target sequence, it creates a mask by checking if the target sequence elements are not equal \n",
    "     to 0 and applies a no-peek mask to prevent attending to future positions.\n",
    "    \"\"\"\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    \"\"\"\n",
    "    The forward function takes source (src) and target (tgt) sequences as input. \n",
    "    It generates source and target masks using the generate_mask function. \n",
    "    The source and target embeddings are obtained by applying dropout to the positional embeddings of the \n",
    "    encoder and decoder embeddings, respectively. \n",
    "    The encoder layers are then applied to the source embeddings to get the encoder output (enc_output). \n",
    "    The decoder layers are applied to the target embeddings along with the encoder output, source mask, \n",
    "    and target mask to get the final decoder output (dec_output). The output is obtained by applying a linear layer to the decoder output.\n",
    "    \"\"\"\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7118e",
   "metadata": {},
   "source": [
    "In this example, we will create a toy dataset for demonstration purposes. In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a00205",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c00c00",
   "metadata": {},
   "source": [
    "We then train the model on the toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce6c8073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.684064865112305\n",
      "Epoch: 2, Loss: 8.544800758361816\n",
      "Epoch: 3, Loss: 8.472278594970703\n",
      "Epoch: 4, Loss: 8.421703338623047\n",
      "Epoch: 5, Loss: 8.367868423461914\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca37964",
   "metadata": {},
   "source": [
    "### Practice for the Week\n",
    "This week, we introduce more complex network architectures that are based on attention machanism and are known as transformers. This architecture makes them particularly useful for tasks such as machine translation, text summarization, and question answering. In the examples above, we show how these architectures work on toy datasets. In this practice, we will use these models to perform machine translation on given samples. Your task is to build a `Dataset` and a `Dataloader` for the English-to-French translation task. The raw code is provided below. You can modify it to suit your needs. Additionally, you are encouraged to implement your own dataset and dataloader with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29abced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72af529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG: {'Good': 0, 'Morning': 1, 'Afternoon': 2, 'Evening': 3}\n",
      "GERMAN: {'Guten': 0, 'Morgen': 1, 'Nachmittag': 2, 'Abend': 3}\n"
     ]
    }
   ],
   "source": [
    "english2german = [('Good Morning', 'Guten Morgen'), \n",
    "                  ('Good Afternoon', 'Guten Nachmittag'),\n",
    "                  ('Good Evening', 'Guten Abend')]\n",
    "eng_vocab2index = {}\n",
    "german_vocab2index = {}\n",
    "\n",
    "for src, tgt in english2german:\n",
    "    for word in src.split():\n",
    "        if word not in eng_vocab2index:\n",
    "            eng_vocab2index[word] = len(eng_vocab2index)\n",
    "    for word in tgt.split():\n",
    "        if word not in german_vocab2index:\n",
    "            german_vocab2index[word] = len(german_vocab2index)\n",
    "\n",
    "print('ENG:', eng_vocab2index)\n",
    "print('GERMAN:', german_vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea088cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tgt: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# convert to numerical data\n",
    "n = len(english2german)\n",
    "input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "for idx, (inp, tgt) in enumerate(english2german):\n",
    "    inp_ids = [eng_vocab2index[word] for word in inp.split(' ')]\n",
    "    tgt_ids = [german_vocab2index[word] for word in tgt.split(' ')]\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "src = torch.LongTensor(input_ids).to(device)\n",
    "tgt = torch.LongTensor(target_ids).to(device)\n",
    "print(f'src: {src}')\n",
    "print(f'tgt: {tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d168062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1357\n",
      "Epoch [2/10], Loss: 0.8162\n",
      "Epoch [3/10], Loss: 0.6153\n",
      "Epoch [4/10], Loss: 0.5152\n",
      "Epoch [5/10], Loss: 0.4607\n",
      "Epoch [6/10], Loss: 0.4420\n",
      "Epoch [7/10], Loss: 0.4216\n",
      "Epoch [8/10], Loss: 0.4256\n",
      "Epoch [9/10], Loss: 0.4259\n",
      "Epoch [10/10], Loss: 0.4209\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "input_size = len(eng_vocab2index)\n",
    "output_size = len(german_vocab2index)\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size , output_size).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(src)\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, tgt)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        tgt.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff55057",
   "metadata": {},
   "source": [
    "### Extension (Not tested in Quiz)\n",
    "You can add start and end tokens to the input sequence to indicate the start and end of the sequence. This can be done by concatenating a start token at the beginning of the sequence and an end token at the end of the sequence. The model can then learn to predict the start and end tokens based on the context of the input sequence. This can help the model to better understand the overall structure of the sequence and improve the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a55eb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Modify the eng_vocab2index and german_vocab2index dictionaries\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Adding the above tokens will increase the vocabulary size by 2\n",
    "# you can also modify MAX_LENGTH to a bigger value\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# Explore other modifications to the code, such as having 'UNK' token in the vocabulary, using a real-world dataset, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
