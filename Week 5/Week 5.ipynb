{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Language Models\n",
    "This week, we introduce neural language models, which are neural networks that handle chronologically ordered data, including 1) RNNs, 2) LSTMs, 3) GRUs, and 4) Bi-RNNs. You will learn more complex model architectures next week.\n",
    "\n",
    "As a quick start, let's import the necessary libraries,  load and pre-process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bi-RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(BiRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 2.2917\n",
      "Epoch [4/10], Loss: 2.0320\n",
      "Epoch [6/10], Loss: 1.7937\n",
      "Epoch [8/10], Loss: 1.5622\n",
      "Epoch [10/10], Loss: 1.3329\n"
     ]
    }
   ],
   "source": [
    "# Simulated data: a many-to-many mapping of sequences to targets\n",
    "# Each sequence is a batch of data: (batch_size, sequence_length)\n",
    "sequence_data = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Corresponding targets\n",
    "target_data = torch.tensor([\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6],\n",
    "    [4, 5, 6, 7]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Vocabulary size and embedding size\n",
    "vocab_size = 10\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "batch_size, seq_len = sequence_data.size()\n",
    "num_epochs = 10\n",
    "\n",
    "# RNN as our model\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, vocab_size)\n",
    " \n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Clear gradient accumulators\n",
    "    output = model(sequence_data)  # Forward pass\n",
    "    loss = criterion(output.view(-1, vocab_size), target_data.view(-1))  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can accelerate your training by using a GPU. Add .cuda() to the model and the input tensor (make sure you have installed the PyTorch GPU version and have a CUDA-enabled GPU.). When you infers using the trained model, you can using .eval() to ensure that the model is in evaluation mode and not training mode .train()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if your device has the GPU support\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): RNN(8, 16, batch_first=True)\n",
       "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "model.cuda()\n",
    "model.train()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice of the Week\n",
    "Below are example code to define a shallow nerual network (MLP) using PyTorch. You can use these as a starting point to build your own models. Noted that these models are randomly initialized. Your task is to set your own task and randomly generate at least 100 samples (X, Y) and train the model on these samples. You can also use available datasets as the training data. Eg., Sentiment analysis is a many-to-one regression task, where the input is a text sentence and the output is a sentiment score. You can use the IMDB dataset for sentiment analysis. Add/replace any layers to improve the performance of the model. \n",
    "\n",
    "Well-trained models can be obtained by training the models on large datasets and fine-tuning the hyperparameters. You will learn more about training such models in the later lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a two layer neural network model\n",
    "class OwnModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(OwnModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = OwnModel(input_size=50, output_size=1, hidden_size=100)\n",
    "\n",
    "# Randomly generate some data. Hint: use torch.randn()\n",
    "\n",
    "# Define the necessary variables for training, such as batch size, input/output size, and training epochs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define a train function\n",
    "def train(model, data, target, optimizer, criterion):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
