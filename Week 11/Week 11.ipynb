{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer based LLMs\n",
    "The transformer based LLMs are widely used in natural language processing tasks such as machine translation, question answering, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use a pre-trained BERT variant model\n",
    "The following examples are shown to use a pre-trained BERT variant model for the mask word prediction (text generation) task. The pre-trained model is downloaded from the Hugging Face Transformers library. Note that the models are not perfect and may not work well in all cases. However, they can provide a good starting point for further research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text data using the corresponding tokenizer for each model\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "sentence = \"NTU's EE6405 course is an awesome [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-base-uncased\n",
      "Sentence: NTU's EE6405 course is an awesome experience.\n",
      "Sentence: NTU's EE6405 course is an awesome course.\n",
      "Sentence: NTU's EE6405 course is an awesome one.\n",
      "Model: roberta-base\n",
      "Sentence: NTU's EE6405 course is an awesome workout.\n",
      "Sentence: NTU's EE6405 course is an awesome experience.\n",
      "Sentence: NTU's EE6405 course is an awesome resource.\n",
      "Model: distilbert-base-uncased\n",
      "Sentence: NTU's EE6405 course is an awesome experience.\n",
      "Sentence: NTU's EE6405 course is an awesome feat.\n",
      "Sentence: NTU's EE6405 course is an awesome attraction.\n"
     ]
    }
   ],
   "source": [
    "for tokenizer, model in zip([bert_tokenizer, roberta_tokenizer, distilbert_tokenizer], \n",
    "                            [bert_model, roberta_model, distilbert_model]):\n",
    "    print('Model:', model.config.name_or_path)\n",
    "    model.to(device).eval()\n",
    "    # A difference in the default setting of the mask token\n",
    "    if model.config.name_or_path == 'roberta-base':\n",
    "        inputs = tokenizer(sentence.replace('[MASK]', '<mask>'), return_tensors=\"pt\")\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs.to(device)).logits\n",
    "\n",
    "        # Extract the mask token index\n",
    "        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Predict the top 5 tokens for the masked position\n",
    "        predicted_token_ids = logits[0, mask_token_index, :].topk(3, dim=1).indices.squeeze()\n",
    "\n",
    "        # Convert token ids to words\n",
    "        predicted_words = [tokenizer.decode([token_id]).strip() for token_id in predicted_token_ids]\n",
    "\n",
    "        # Print the predictions\n",
    "        for word in predicted_words:\n",
    "            print(f\"Sentence: {sentence.replace('[MASK]', word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence with the best prediction:\n",
      "NTU's EE6405 course is an awesome resource.\n",
      "Predicted word:  resource, Score: 0.2637\n",
      "Predicted word:  experience, Score: 0.0283\n",
      "Predicted word:  example, Score: 0.0259\n",
      "Predicted word:  tool, Score: 0.0250\n",
      "Predicted word:  addition, Score: 0.0220\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can call the model with a pipeline\n",
    "from transformers import pipeline\n",
    "distilroberta_model = pipeline('fill-mask', model='distilroberta-base')\n",
    "predictions = distilroberta_model(sentence.replace('[MASK]', '<mask>'))\n",
    "print()\n",
    "print(f'Sentence with the best prediction:\\n{sentence.replace(\"[MASK]\", predictions[0][\"token_str\"].strip())}')\n",
    "for prediction in predictions:\n",
    "    print(f\"Predicted word: {prediction['token_str']}, Score: {prediction['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use a pre-trained generative model\n",
    "In this section, we will demonstrate how to use a pre-trained generative model for language translation, text completion, and multiple choice question answering. Note that the models are not perfect and may not work well in all cases. However, they can provide a good starting point for further research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large', \n",
    "                                                     forced_bos_token_id=0) # takes a while to load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-base\n",
      "Input: translate English to German: Good morning. How are you?\n",
      "Output: Guten Morgen, wie sind Sie?\n",
      "Model: facebook/bart-large\n",
      "Input: We study NLP and learn to <mask> in EE6405.\n",
      "Output: We study NLP and learn to use it in EE6405.\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"translate English to German: Good morning. How are you?\"\n",
    "sentence2 = \"We study NLP and learn to <mask> in EE6405.\"\n",
    "for tokenizer, model, sent in zip([t5_tokenizer, bart_tokenizer], \n",
    "                                  [t5_model, bart_model],\n",
    "                                  [sentence1, sentence2]):\n",
    "    print('Model:', model.config.name_or_path)\n",
    "    model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "        output = model.generate(input_ids=input_ids.to(device), max_length=50, num_beams=2, early_stopping=True)\n",
    "        print('Input:', sent)\n",
    "        print('Output:', tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForMultipleChoice were not initialized from the model checkpoint at xlnet/xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLNetForMultipleChoice\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "model = XLNetForMultipleChoice.from_pretrained('xlnet/xlnet-base-cased').to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Natural Language Processing\n"
     ]
    }
   ],
   "source": [
    "question = \"What does NLP stands for\"\n",
    "answers = [\"Computer Vision\", \"Natural Language Processing\"]\n",
    "encoding = tokenizer([question]*len(answers), answers, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}) \n",
    "    logits = outputs.logits\n",
    "    softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
    "    index = torch.argmax(softmax, dim = -1)\n",
    "    print(\"Answer:\", answers[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tune a pre-trained transformer model for sequence classification\n",
    "Fine-tuning a pre-trained transformer model for language modeling is a popular approach for language modeling. In this notebook, we will use the Hugging Face transformers library to fine-tune transformer-based language models on the newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "groups = ['alt.atheism', 'rec.sport.baseball', 'sci.space']\n",
    "\n",
    "# Load dataset\n",
    "# newsgroups_data = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, categories=groups)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 1 # set this to a higher number in actual fine-tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fine-tune a pretrained BertForSequenceClassification model\n",
    "Note: If your device does not have cuda-enable, it is recommended to use a smaller dataset and a smaller training epoch to have a taste of the fine-tune. The fine-tune layers are, by default, freeze except the the classification layer to save computational resources. The training epoch is set to 1 for demonstration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_data = tokenizer(newsgroups_data.data, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "X = encoded_data['input_ids']\n",
    "y = torch.tensor(newsgroups_data.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 epochs: 1.1006632194244603\n"
     ]
    }
   ],
   "source": [
    "# Define BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', torch_dtype = torch.float16, num_labels=len(np.unique(newsgroups_data.target)))\n",
    "\n",
    "# Freeze all layers except the final classification layer\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define optimizer, set a small lr for fine-tune\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=2e-5)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=data.to(device), labels=target.to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'{epoch+1}/{num_epochs} epochs: {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.37050359712230213\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicts = []\n",
    "    truth = []\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(input_ids=data.to(device), labels=target.to(device))\n",
    "        predicts.append(torch.argmax(outputs.logits, dim=1).detach().cpu().numpy())\n",
    "        truth.append(target.numpy())\n",
    "    accuracy = accuracy_score(np.concatenate(truth), np.concatenate(predicts))\n",
    "    print(f'Test Set Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine-tune a pretrained GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure the tokenizer uses a pad token\n",
    "\n",
    "encoded_data = tokenizer(newsgroups_data.data, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "X = encoded_data['input_ids']\n",
    "y = torch.tensor(newsgroups_data.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 epochs: 1.5180492137404655\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 model with a classification head\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=len(np.unique(newsgroups_data.target)))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Freeze all layers except the last transformer block and the language model head\n",
    "for name, param in model.named_parameters():\n",
    "    if 'h.11' not in name and 'ln_f' not in name and 'lm_head' not in name:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Setup an optimizer to only update the un-frozen parameters\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=data.to(device), labels=target.to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'{epoch+1}/{num_epochs} epochs: {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.5539568345323741\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicts = []\n",
    "    truth = []\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(input_ids=data.to(device), labels=target.to(device))\n",
    "        predicts.append(torch.argmax(outputs.logits, dim=1).detach().cpu().numpy())\n",
    "        truth.append(target.numpy())\n",
    "    accuracy = accuracy_score(np.concatenate(truth), np.concatenate(predicts))\n",
    "    print(f'Test Set Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice for the Week\n",
    "You have learnt to fine-tune pre-trained models for your own use cases. However, there are many pre-trained models available on the internet that can be used for a variety of tasks. Let's practice fine-tuning more advanced pre-trained models. In this task, you are recommended to first implement a simple fine-tuning task using a pre-trained model, such as a Bert (without looking at the code above).\n",
    "\n",
    "Here's the practice: Unless every input sequence is precisely the same length without any padding, it is also necessary to include attention mask which is missing from the demonstration of fine-tuning the BERT model in the notebook. The attention mask is used to mask out the padding tokens in the input sequence, so that the model does not pay attention to them. \n",
    "\n",
    "Then, you can try to fine-tune a more advanced pre-trained model, such as RoBERTa, BART, XLNet, for your own task. \n",
    "\n",
    "Alternatively, try to fine-tune a model to classify the newsgroup other than the two examples provided in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.keys()\n",
    "# outputs = model(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
