{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning\n",
    "It is important to tune the hyperparameters of a machine learning model to get the best performance. In this notebook, we will use the 20 newgroups dataset to tune our model's hyperparameters. It might take some time to download the dataset and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "groups = ['alt.atheism', 'rec.sport.baseball', 'sci.space']\n",
    "\n",
    "# Load the 20 newsgroups dataset, we set it with only 3 categories to save time, you can change it to all categories by removing the 'categories' parameter\n",
    "train_data = fetch_20newsgroups(shuffle=True, random_state=42, categories=groups)\n",
    "\n",
    "# Convert text data into a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_data.data)\n",
    "y = train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1670, 27427), (1670,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. K-Fold Cross-Validation and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Score: 0.9934383662042003\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0]}\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "classifier = MultinomialNB()\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model using cross-validation\n",
    "cv_scores = cross_val_score(best_classifier, X, y, cv=5, scoring='f1_macro')\n",
    "mean_cv_score = cv_scores.mean()\n",
    "print(f'Mean Cross-Validation Score: {mean_cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.985030</td>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.993413</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.988024</td>\n",
       "      <td>0.982036</td>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.991018</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "      <td>0.994012</td>\n",
       "      <td>0.988024</td>\n",
       "      <td>0.985030</td>\n",
       "      <td>0.997006</td>\n",
       "      <td>0.991018</td>\n",
       "      <td>0.991018</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_alpha  \\\n",
       "0       0.003577      0.000796         0.001100        0.000193          0.1   \n",
       "1       0.003570      0.000204         0.000694        0.000241          0.5   \n",
       "2       0.003577      0.000371         0.000596        0.000197          1.0   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.1}           0.994012           0.994012           0.985030   \n",
       "1  {'alpha': 0.5}           0.994012           0.988024           0.982036   \n",
       "2  {'alpha': 1.0}           0.994012           0.988024           0.985030   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.997006           0.997006         0.993413        0.004400   \n",
       "1           0.997006           0.994012         0.991018        0.005356   \n",
       "2           0.997006           0.991018         0.991018        0.004234   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                2  \n",
       "2                2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set F1 Score: 0.9751950960932954\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "test_data = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=groups)\n",
    "X_test = vectorizer.transform(test_data.data)\n",
    "y_test = test_data.target\n",
    "\n",
    "predictions = best_classifier.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    "print(f'Test Set F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameters of a neural network\n",
    "Note that you need to modify the hyperparameters if your device can't train the network efficiently in this notebook (to get a taste of the workflow). The training pipeline is built using a toy classifier and a small dataset, so it may not be the best choice for a real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple toy classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # extract only the last time step\n",
    "        return self.fc(output[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split1\n",
      "Loss: 1.0938678148263943\n",
      "F1 score: 0.1762114537444934\n",
      "Split2\n",
      "Loss: 1.0923432218814324\n",
      "F1 score: 0.17185185185185184\n",
      "Split3\n",
      "Loss: 1.0945039937596122\n",
      "F1 score: 0.1729490022172949\n",
      "Split4\n",
      "Loss: 1.098829667725249\n",
      "F1 score: 0.16853094705443697\n",
      "Split5\n",
      "Loss: 1.0940548424235361\n",
      "F1 score: 0.1762114537444934\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = len(np.unique(y))\n",
    "batch_size = 8\n",
    "training_epochs = 3\n",
    "\n",
    "# Perform 5-fold cross-validation with early stop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Convert data to tensors\n",
    "    X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.long)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val.toarray(), dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=1)\n",
    "    \n",
    "    model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(training_epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.to(device))\n",
    "            loss = criterion(output.to(device), target.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        truth = []\n",
    "        preds = []\n",
    "        val_loss = []\n",
    "        for data, target in val_loader:\n",
    "            output = model(data.to(device))\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            preds.append(predicted.detach().cpu().numpy())\n",
    "            truth.append(target.numpy())\n",
    "            val_loss.append(criterion(output.to(device), target.to(device)).item())\n",
    "        f1 = f1_score(np.concatenate(truth), np.concatenate(preds), average='macro')\n",
    "        print(f'Split{i+1}\\nLoss: {np.mean(val_loss)}\\nF1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.17464539007092197\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    truth = []\n",
    "    preds = []\n",
    "    for data, target in test_loader:\n",
    "        output = model(data.to(device))\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        preds.append(predicted.detach().cpu().numpy())\n",
    "        truth.append(target.numpy())\n",
    "    f1 = f1_score(np.concatenate(truth), np.concatenate(preds), average='macro')\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice for the Week\n",
    "You have learnt ways to select the hyper-parameters for a machine learning model. Here's the early stopping function based on the loss function. When the difference between the current and the previous loss is less than `delta` (`mode`=`min`), the early stopping function will restore the best network weights and train until `n` patience epochs, before the training stopped. You can modify the early stopping function to suit your needs or replace it with the PyTorch EarlyStopping class from `ignite.handlers`. Apply it to training pipeline. The LSTMClassifier is a very simple toy classifier, improve it by having a better network architecture. Define the hyperparameters that affects the LSTMClassifier and improve the overall f1 score of the test data (20 categories). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.handlers import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, restore_best_weight=True, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.restore_best_weight = restore_best_weight\n",
    "        self.other_metric = None\n",
    "        self.loop = -1\n",
    "        self.mode = mode\n",
    "        self.best_state = None\n",
    "\n",
    "    def __call__(self, model, val_loss, loop, other_metric=None):\n",
    "        if self.mode == 'min':\n",
    "            score = -val_loss\n",
    "        else:\n",
    "            score = val_loss\n",
    "            \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.other_metric = other_metric\n",
    "            self.loop = loop\n",
    "            self.best_state = model.state_dict()\n",
    "            self.update_loss(val_loss)\n",
    "            return True\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print('Restore best weight...')\n",
    "            if self.restore_best_weight:\n",
    "                model.load_state_dict(self.best_state)\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print('')\n",
    "                print(f'EarlyStopping at {loop}! Val Loss:{self.val_loss_min}. Other Metrics:{self.other_metric} from epoch {self.loop}')\n",
    "            return False\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.other_metric = other_metric\n",
    "            self.loop = loop\n",
    "            self.best_state = model.state_dict()\n",
    "            self.update_loss(val_loss)\n",
    "            self.counter = 0\n",
    "            return True\n",
    "\n",
    "    def update_loss(self, val_loss):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model checkpoint...')\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "stopper = EarlyStopping(patience=2, delta=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: modify the network architecture. eg., add dropout, activation functions, more layers, number of training epochs, batch size, etc.\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # extract only the last time step\n",
    "        return self.fc(output[:, -1, :])\n",
    "    \n",
    "# Build the training pipeline as your preference\n",
    "def train():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
